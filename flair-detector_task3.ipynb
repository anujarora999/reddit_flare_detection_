{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This task involves creating a Reddit Flair Detector. I have first \\npreprocessed the data extracted from r/india.\\nAfter preprocessing, I tested the data with 3 classifier, Naive Bayees,\\nSupport Vector Machine and Logistic Regression. The accuracy of Logistic\\nRegression was the highest, so I made my model using that. I uploaded my dataset \\non mongodb using atlas and studio 3T. I used that database test the models. \\nI saved my final model, logistic regression in .sav file, to be able to use it\\nwhile predicting.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This task involves creating a Reddit Flair Detector. I have first \n",
    "preprocessed the data extracted from r/india.\n",
    "After preprocessing, I tested the data with 3 classifier, Naive Bayees,\n",
    "Support Vector Machine and Logistic Regression. The accuracy of Logistic\n",
    "Regression was the highest, so I made my model using that. I uploaded my dataset \n",
    "on mongodb using atlas and studio 3T. I used that database test the models. \n",
    "I saved my final model, logistic regression in .sav file, to be able to use it\n",
    "while predicting.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "import pymongo\n",
    "import praw\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Removing the stopwords'''\n",
    "def remove_stopwords(text):\n",
    "\tfor w in text:\n",
    "\t\tif w == \"\\n\":\n",
    "\t\t\tw= \" \"\n",
    "\twords = [w for w in text if w not in stopwords.words(\"english\")]\n",
    "\treturn words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Stemming using Porter Stemmer. It involves converting all words to their root word'''\n",
    "ps= PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "\tlis = []\n",
    "\tfor t in text:\n",
    "\t\tt = str(t)\n",
    "\t\taux_l = \"\"\n",
    "\t\twords = []\n",
    "\t\twords = word_tokenize(t)\n",
    "\t\tfor w in words:\n",
    "\t\t\trootword = ps.stem(w)\n",
    "\t\t\tif(rootword.isalnum()):\n",
    "\t\t\t\taux_l = aux_l+ rootword+ \" \"\n",
    "\t\tlis.append(str(aux_l))\n",
    "\treturn lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Reading the database to preprocess, convert items to use and preprocess in a list'''\n",
    "df = pd.read_csv('testset.csv')\n",
    "df.fillna(\"\")\n",
    "title= df[\"title\"].tolist()\n",
    "body= df[\"body\"].tolist()\n",
    "comment = df[\"comments\"].tolist()\n",
    "#print(\"-----------------\")\n",
    "#print(comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Removing stopwords from title, body and comments'''\n",
    "t_remove = remove_stopwords(title)\n",
    "b_remove = remove_stopwords(body)\n",
    "c_remove = remove_stopwords(comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Converting to dataframe and storing in a csv file'''\n",
    "df[\"title\"] = t_remove\n",
    "df[\"body\"] = b_remove\n",
    "df[\"comments\"] = c_remove\n",
    "df.to_csv(\"test_stopwords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Reading csv file with remove stopwords and applying stemming on them.'''\n",
    "df = pd.read_csv('test_stopwords.csv')\n",
    "df.fillna(\"\")\n",
    "title= df[\"title\"].tolist()\n",
    "body= df[\"body\"].tolist()\n",
    "comment = df[\"comments\"].tolist()\n",
    "t_remove_stem= stemming(title)\n",
    "b_remove_stem = stemming(body)\n",
    "c_remove_stem =  stemming(comment)\n",
    "#c_remove_stem = stemming(c_remove)\n",
    "#print(t_remove_stem)\n",
    "#print(\"---------------------------------\")\n",
    "#print(b_remove_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Converting stemmed data to data frame'''\n",
    "t1 = pd.DataFrame(t_remove_stem)\n",
    "b1 = pd.DataFrame(b_remove_stem)\n",
    "c1 = pd.DataFrame(c_remove_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Saving the data into a csv file'''\n",
    "df[\"title\"] = t1\n",
    "df[\"body\"] = b1\n",
    "df[\"comments\"] = c1\n",
    "df.to_csv(\"preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''declaring list of target flairs for model testing'''\n",
    "flairs = [\"Scheduled\", \"Politics\", \"Photography\",\"Policy/Economy\",\n",
    "         \"Sports\",\"Non-Political\",\"Science/Technology\",\"Food\",\n",
    "            \"Business/Finance\",\"Coronavirus\",\"Megathread\",\"CAA-NRC\",\"[R]eddiquette\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.head())\n",
    "#y = df[\"flair\"]\n",
    "#x = df.drop(\"flair\", axis = 1)\n",
    "#print(x)\n",
    "#print(y)\n",
    "#x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25)\n",
    "#print(x_train)\n",
    "#print(x_train.shape, x_test.shape)\n",
    "#print(x_test)\n",
    "#train = x_train.to_csv(\"train.csv\")\n",
    "#test = x_test.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testing the data along 3 classifiers: Naive Bayees,\\nSupport Vector Machine and Logistic Regression. Used sklearn library for the same'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Testing the data along 3 classifiers: Naive Bayees,\n",
    "Support Vector Machine and Logistic Regression. Used sklearn library for the same'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Naive Bayees Classfier'''\n",
    "def nb_classifier(X_train, X_test, y_train, y_test):\n",
    "\tfrom sklearn.naive_bayes import MultinomialNB\n",
    "\tnb = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB()),])\n",
    "\tnb.fit(X_train, y_train)\n",
    "\ty_pred = nb.predict(X_test)\n",
    "\tnew_l = []\n",
    "\t\"\"\"for i in flairs:\n",
    "\t\t\t\t\tif(i in y_test):\n",
    "\t\t\t\t\t\tnew_l.append(i)\"\"\"\n",
    "\tprint(\"Results of Naive Bayes Classifier\")\n",
    "\tprint('accuracy ' + str(accuracy_score(y_pred, y_test)))\n",
    "\tprint(\"Classification Report is: \")\n",
    "\tprint(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SVM Classifier'''\n",
    "def svm_classifier(X_train,X_test,y_train,y_test):\n",
    "\tfrom sklearn.linear_model import SGDClassifier\n",
    "\tsvm = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', SGDClassifier()),])\n",
    "\tsvm.fit(X_train, y_train)\n",
    "\ty_pred = svm.predict(X_test)\n",
    "\tnew_l = []\n",
    "\t\"\"\"for i in flairs:\n",
    "\t\t\t\t\tif(i in y_test):\n",
    "\t\t\t\t\t\tnew_l.append(i)\"\"\"\n",
    "\n",
    "\tprint(\"Results of Support Vector Machine Classifier\")\n",
    "\tprint('accuracy ' + str(accuracy_score(y_pred, y_test)))\n",
    "\tprint(\"Classification Report is: \")\n",
    "\tprint(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Logistic Regression Classifier'''\n",
    "def logreg_classifier(X_train,X_test,y_train,y_test):\n",
    "\tfrom sklearn.linear_model import LogisticRegression\n",
    "\tlgr = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', LogisticRegression()),])\n",
    "\tlgr.fit(X_train, y_train)\n",
    "\tnew_l = []\n",
    "\tfilename = 'finalized_model.sav'\n",
    "\tpickle.dump(lgr, open(filename, 'wb'))\n",
    "\ty_pred = lgr.predict(X_test)\n",
    "\t\n",
    "\tprint(\"Results of Logistic Regression\")\n",
    "\tprint('accuracy ' + str(accuracy_score(y_pred, y_test)))\n",
    "\tprint(\"Classification Report is: \")\n",
    "\tprint(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dividing the data into training and test dataset'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Dividing the data into training and test dataset'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_nb(X,y):\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\tnb_classifier(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_svm(X,y):\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\tsvm_classifier(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_lr(X,y):\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\tlogreg_classifier(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Reading the data from MongoDB. Used pymongo to do the same.'''\n",
    "#df = pd.read_csv('preprocessed.csv')\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client['Midaas']\n",
    "preprocess = db.preprocessed\n",
    "df = pd.DataFrame(list(preprocess.find()))\n",
    "df[\"flair\"] = df[\"flair\"].values.astype('U')\n",
    "df[\"comments\"] = df[\"comments\"].values.astype('U')\n",
    "df[\"title\"] = df[\"title\"].values.astype('U')\n",
    "df[\"body\"] = df[\"body\"].values.astype('U')\n",
    "fl = df[\"flair\"].tolist()\n",
    "com = df[\"comments\"].tolist()\n",
    "tit = df[\"title\"].tolist()\n",
    "bod = df[\"body\"].tolist()\n",
    "combined = (df[\"comments\"]+ df[\"title\"]+df[\"body\"]).tolist()\n",
    "#df = df.assign(combine = combined)\n",
    "#com = df.combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flair Detection using Title as Feature\n",
      "Results of Naive Bayes Classifier\n",
      "accuracy 0.5422374429223744\n",
      "Classification Report is: \n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         Scheduled       0.60      0.36      0.45        77\n",
      "          Politics       0.50      0.84      0.63        70\n",
      "       Photography       0.51      0.45      0.48        84\n",
      "    Policy/Economy       0.70      0.62      0.66        73\n",
      "            Sports       0.00      0.00      0.00         1\n",
      "     Non-Political       0.22      0.31      0.26        67\n",
      "Science/Technology       0.87      0.80      0.83        74\n",
      "              Food       0.37      0.45      0.40        74\n",
      "  Business/Finance       0.36      0.49      0.41        71\n",
      "       Coronavirus       0.90      0.99      0.94        77\n",
      "        Megathread       0.46      0.43      0.44        75\n",
      "           CAA-NRC       0.92      0.53      0.67        68\n",
      "     [R]eddiquette       0.42      0.20      0.27        65\n",
      "\n",
      "          accuracy                           0.54       876\n",
      "         macro avg       0.53      0.50      0.50       876\n",
      "      weighted avg       0.57      0.54      0.54       876\n",
      "\n",
      "Flair Detection using Body as Feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Naive Bayes Classifier\n",
      "accuracy 0.2180365296803653\n",
      "Classification Report is: \n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         Scheduled       0.33      0.01      0.03        69\n",
      "          Politics       0.00      0.00      0.00        79\n",
      "       Photography       1.00      0.01      0.03        78\n",
      "    Policy/Economy       1.00      0.15      0.26        82\n",
      "            Sports       0.00      0.00      0.00         1\n",
      "     Non-Political       0.00      0.00      0.00        67\n",
      "Science/Technology       0.10      1.00      0.19        64\n",
      "              Food       0.00      0.00      0.00        77\n",
      "  Business/Finance       0.00      0.00      0.00        66\n",
      "       Coronavirus       0.93      0.94      0.93        82\n",
      "        Megathread       0.50      0.13      0.21        75\n",
      "           CAA-NRC       0.00      0.00      0.00        69\n",
      "     [R]eddiquette       0.18      0.39      0.24        67\n",
      "\n",
      "          accuracy                           0.22       876\n",
      "         macro avg       0.31      0.20      0.15       876\n",
      "      weighted avg       0.36      0.22      0.17       876\n",
      "\n",
      "Flair Detection using Comments as Feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Naive Bayes Classifier\n",
      "accuracy 0.2100456621004566\n",
      "Classification Report is: \n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         Scheduled       0.67      0.03      0.05        74\n",
      "          Politics       1.00      0.01      0.02        80\n",
      "       Photography       0.31      0.30      0.30        77\n",
      "    Policy/Economy       1.00      0.08      0.14        78\n",
      "            Sports       0.00      0.00      0.00         3\n",
      "     Non-Political       0.00      0.00      0.00        68\n",
      "Science/Technology       0.89      0.12      0.21        69\n",
      "              Food       1.00      0.01      0.02        87\n",
      "  Business/Finance       0.18      0.86      0.29        74\n",
      "       Coronavirus       0.19      0.99      0.32        75\n",
      "        Megathread       0.00      0.00      0.00        66\n",
      "           CAA-NRC       0.00      0.00      0.00        54\n",
      "     [R]eddiquette       0.29      0.07      0.11        71\n",
      "\n",
      "          accuracy                           0.21       876\n",
      "         macro avg       0.43      0.19      0.11       876\n",
      "      weighted avg       0.49      0.21      0.13       876\n",
      "\n",
      "Flair Detection using Combined Features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Naive Bayes Classifier\n",
      "accuracy 0.3047945205479452\n",
      "Classification Report is: \n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         Scheduled       0.50      0.08      0.14        60\n",
      "          Politics       0.80      0.22      0.35        72\n",
      "       Photography       0.79      0.15      0.26        72\n",
      "    Policy/Economy       1.00      0.20      0.34        79\n",
      "            Sports       0.00      0.00      0.00         2\n",
      "     Non-Political       0.09      0.01      0.02        70\n",
      "Science/Technology       1.00      0.37      0.54        79\n",
      "              Food       0.29      0.14      0.19        65\n",
      "  Business/Finance       0.17      0.93      0.28        74\n",
      "       Coronavirus       0.34      1.00      0.51        84\n",
      "        Megathread       0.50      0.04      0.07        79\n",
      "           CAA-NRC       1.00      0.24      0.39        74\n",
      "     [R]eddiquette       0.10      0.09      0.09        66\n",
      "\n",
      "          accuracy                           0.30       876\n",
      "         macro avg       0.51      0.27      0.24       876\n",
      "      weighted avg       0.56      0.30      0.27       876\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "'''Printing results of the final model for title, body,comments and combined'''\n",
    "print(\"Flair Detection using Title as Feature\")\n",
    "train_test_nb(tit,fl)\n",
    "print(\"Flair Detection using Body as Feature\")\n",
    "train_test_nb(bod,fl)\n",
    "print(\"Flair Detection using Comments as Feature\")\n",
    "train_test_nb(com,fl)\n",
    "print(\"Flair Detection using Combined Features\")\n",
    "train_test_nb(combined,fl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flair Detection using Title as Feature\n",
      "Results of Support Vector Machine Classifier\n",
      "accuracy 0.5159817351598174\n",
      "Classification Report is: \n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         Scheduled       0.41      0.44      0.42        72\n",
      "          Politics       0.71      0.66      0.68        90\n",
      "       Photography       0.42      0.59      0.49        71\n",
      "    Policy/Economy       0.59      0.60      0.59        68\n",
      "            Sports       0.00      0.00      0.00         1\n",
      "     Non-Political       0.24      0.22      0.23        69\n",
      "Science/Technology       0.74      0.73      0.74        63\n",
      "              Food       0.33      0.33      0.33        81\n",
      "  Business/Finance       0.33      0.39      0.36        74\n",
      "       Coronavirus       0.93      0.97      0.95        72\n",
      "        Megathread       0.40      0.29      0.34        75\n",
      "           CAA-NRC       0.79      0.69      0.74        67\n",
      "     [R]eddiquette       0.37      0.32      0.34        73\n",
      "\n",
      "          accuracy                           0.52       876\n",
      "         macro avg       0.48      0.48      0.48       876\n",
      "      weighted avg       0.52      0.52      0.51       876\n",
      "\n",
      "Flair Detection using Body as Feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Support Vector Machine Classifier\n",
      "accuracy 0.2271689497716895\n",
      "Classification Report is: \n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         Scheduled       0.62      0.13      0.22        77\n",
      "          Politics       0.31      0.06      0.10        68\n",
      "       Photography       0.29      0.05      0.09        76\n",
      "    Policy/Economy       0.75      0.28      0.40        76\n",
      "            Sports       0.00      0.00      0.00         3\n",
      "     Non-Political       0.30      0.04      0.07        82\n",
      "Science/Technology       0.00      0.00      0.00        67\n",
      "              Food       0.44      0.12      0.18        69\n",
      "  Business/Finance       0.47      0.11      0.18        71\n",
      "       Coronavirus       0.92      0.93      0.93        74\n",
      "        Megathread       0.52      0.21      0.30        77\n",
      "           CAA-NRC       1.00      0.06      0.11        67\n",
      "     [R]eddiquette       0.08      0.75      0.14        69\n",
      "\n",
      "          accuracy                           0.23       876\n",
      "         macro avg       0.44      0.21      0.21       876\n",
      "      weighted avg       0.48      0.23      0.23       876\n",
      "\n",
      "Flair Detection using Comments as Feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Support Vector Machine Classifier\n",
      "accuracy 0.5079908675799086\n",
      "Classification Report is: \n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         Scheduled       0.38      0.38      0.38        74\n",
      "          Politics       0.67      0.67      0.67        81\n",
      "       Photography       0.47      0.31      0.37        71\n",
      "    Policy/Economy       0.64      0.63      0.64        75\n",
      "            Sports       0.00      0.00      0.00         3\n",
      "     Non-Political       0.45      0.23      0.30        93\n",
      "Science/Technology       0.62      0.76      0.68        62\n",
      "              Food       0.22      0.48      0.30        69\n",
      "  Business/Finance       0.47      0.59      0.52        73\n",
      "       Coronavirus       0.94      0.97      0.95        65\n",
      "        Megathread       0.42      0.33      0.37        76\n",
      "           CAA-NRC       0.77      0.53      0.63        64\n",
      "     [R]eddiquette       0.45      0.40      0.42        70\n",
      "\n",
      "          accuracy                           0.51       876\n",
      "         macro avg       0.50      0.48      0.48       876\n",
      "      weighted avg       0.53      0.51      0.51       876\n",
      "\n",
      "Flair Detection using Combined Features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Support Vector Machine Classifier\n",
      "accuracy 0.6335616438356164\n",
      "Classification Report is: \n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         Scheduled       0.41      0.51      0.45        69\n",
      "          Politics       0.78      0.66      0.72        88\n",
      "       Photography       0.61      0.67      0.64        70\n",
      "    Policy/Economy       0.79      0.77      0.78        83\n",
      "            Sports       0.00      0.00      0.00         4\n",
      "     Non-Political       0.39      0.25      0.31        72\n",
      "Science/Technology       0.79      0.86      0.82        69\n",
      "              Food       0.58      0.49      0.54        85\n",
      "  Business/Finance       0.45      0.62      0.52        60\n",
      "       Coronavirus       0.89      0.92      0.90        76\n",
      "        Megathread       0.51      0.53      0.52        72\n",
      "           CAA-NRC       0.91      0.82      0.86        65\n",
      "     [R]eddiquette       0.47      0.54      0.50        63\n",
      "\n",
      "          accuracy                           0.63       876\n",
      "         macro avg       0.58      0.59      0.58       876\n",
      "      weighted avg       0.64      0.63      0.63       876\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Flair Detection using Title as Feature\")\n",
    "train_test_svm(tit,fl)\n",
    "print(\"Flair Detection using Body as Feature\")\n",
    "train_test_svm(bod,fl)\n",
    "print(\"Flair Detection using Comments as Feature\")\n",
    "train_test_svm(com,fl)\n",
    "print(\"Flair Detection using Combined Features\")\n",
    "train_test_svm(combined,fl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flair Detection using Title as Feature\n",
      "Results of Logistic Regression\n",
      "accuracy 0.5422374429223744\n",
      "Classification Report is: \n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         Scheduled       0.45      0.46      0.45        68\n",
      "          Politics       0.69      0.66      0.67        86\n",
      "       Photography       0.49      0.56      0.52        80\n",
      "    Policy/Economy       0.66      0.61      0.63        75\n",
      "            Sports       0.00      0.00      0.00         3\n",
      "     Non-Political       0.32      0.25      0.28        77\n",
      "Science/Technology       0.69      0.82      0.75        62\n",
      "              Food       0.46      0.55      0.50        74\n",
      "  Business/Finance       0.29      0.32      0.30        73\n",
      "       Coronavirus       0.99      0.92      0.95        79\n",
      "        Megathread       0.42      0.44      0.43        72\n",
      "           CAA-NRC       0.71      0.62      0.66        58\n",
      "     [R]eddiquette       0.36      0.30      0.33        69\n",
      "\n",
      "          accuracy                           0.54       876\n",
      "         macro avg       0.50      0.50      0.50       876\n",
      "      weighted avg       0.54      0.54      0.54       876\n",
      "\n",
      "Flair Detection using Body as Feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Logistic Regression\n",
      "accuracy 0.2226027397260274\n",
      "Classification Report is: \n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         Scheduled       0.45      0.07      0.12        70\n",
      "          Politics       1.00      0.01      0.03        71\n",
      "       Photography       0.00      0.00      0.00        74\n",
      "    Policy/Economy       0.65      0.28      0.39        78\n",
      "            Sports       0.00      0.00      0.00         3\n",
      "     Non-Political       0.17      0.03      0.05        70\n",
      "Science/Technology       0.10      0.97      0.17        62\n",
      "              Food       0.38      0.09      0.15        86\n",
      "  Business/Finance       0.50      0.01      0.03        72\n",
      "       Coronavirus       0.87      0.92      0.90        75\n",
      "        Megathread       0.46      0.14      0.21        79\n",
      "           CAA-NRC       0.00      0.00      0.00        69\n",
      "     [R]eddiquette       0.24      0.24      0.24        67\n",
      "\n",
      "          accuracy                           0.22       876\n",
      "         macro avg       0.37      0.21      0.18       876\n",
      "      weighted avg       0.41      0.22      0.19       876\n",
      "\n",
      "Flair Detection using Comments as Feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Logistic Regression\n",
      "accuracy 0.5228310502283106\n",
      "Classification Report is: \n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         Scheduled       0.49      0.45      0.47        71\n",
      "          Politics       0.65      0.64      0.64        77\n",
      "       Photography       0.30      0.52      0.38        71\n",
      "    Policy/Economy       0.65      0.67      0.66        70\n",
      "            Sports       0.00      0.00      0.00         1\n",
      "     Non-Political       0.41      0.36      0.38        73\n",
      "Science/Technology       0.50      0.81      0.62        62\n",
      "              Food       0.39      0.49      0.43        72\n",
      "  Business/Finance       0.49      0.60      0.54        73\n",
      "       Coronavirus       0.92      0.91      0.92        79\n",
      "        Megathread       0.63      0.23      0.34        82\n",
      "           CAA-NRC       0.93      0.35      0.51        77\n",
      "     [R]eddiquette       0.32      0.29      0.31        68\n",
      "\n",
      "          accuracy                           0.52       876\n",
      "         macro avg       0.52      0.49      0.48       876\n",
      "      weighted avg       0.57      0.52      0.52       876\n",
      "\n",
      "Flair Detection using Combined Features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Logistic Regression\n",
      "accuracy 0.6381278538812786\n",
      "Classification Report is: \n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         Scheduled       0.45      0.55      0.49        62\n",
      "          Politics       0.75      0.74      0.75        70\n",
      "       Photography       0.52      0.67      0.58        72\n",
      "    Policy/Economy       0.79      0.70      0.74        76\n",
      "            Sports       0.00      0.00      0.00         1\n",
      "     Non-Political       0.47      0.26      0.33        77\n",
      "Science/Technology       0.72      0.92      0.81        74\n",
      "              Food       0.48      0.45      0.47        69\n",
      "  Business/Finance       0.48      0.68      0.56        81\n",
      "       Coronavirus       0.91      0.96      0.94        76\n",
      "        Megathread       0.69      0.57      0.62        76\n",
      "           CAA-NRC       0.84      0.78      0.81        69\n",
      "     [R]eddiquette       0.56      0.38      0.46        73\n",
      "\n",
      "          accuracy                           0.64       876\n",
      "         macro avg       0.59      0.59      0.58       876\n",
      "      weighted avg       0.64      0.64      0.63       876\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Flair Detection using Title as Feature\")\n",
    "train_test_lr(tit,fl)\n",
    "print(\"Flair Detection using Body as Feature\")\n",
    "train_test_lr(bod,fl)\n",
    "print(\"Flair Detection using Comments as Feature\")\n",
    "train_test_lr(com,fl)\n",
    "print(\"Flair Detection using Combined Features\")\n",
    "train_test_lr(combined,fl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = \"\"\n",
    "client_secret = \"\"\n",
    "user_agent= \"\"\n",
    "username = \"\"\n",
    "password = \"\"\n",
    "\n",
    "reddit = praw.Reddit(client_id = client_id,client_secret=client_secret,\n",
    "                    user_agent = user_agent,username= username,\n",
    "                     password=password)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def actual(url):\n",
    "    submission = reddit.submission(url = url)\n",
    "    tr = submission.link_flair_text\n",
    "    return tr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(url):\n",
    "    s = \"\"\n",
    "    submission = reddit.submission(url=url)\n",
    "    #print(submission.title)\n",
    "    posts = {\"title\":[], \"body\":[], \"comments\": [],\"combine\": []}\n",
    "    posts[\"title\"] = submission.title\n",
    "    posts[\"body\"] = submission.selftext\n",
    "    comment_list = []\n",
    "    submission.comments.replace_more(limit = 0)\n",
    "    for comment in submission.comments:\n",
    "        comment_list.append(comment.body)\n",
    "    posts[\"comments\"] = comment_list\n",
    "    c_remove = []\n",
    "    for i in posts[\"comments\"]:\n",
    "    \tc = remove_stopwords(i)\n",
    "    \tc_remove.append(c)\n",
    "    t_remove = remove_stopwords(posts[\"title\"])\n",
    "    b_remove = remove_stopwords(posts[\"body\"])\n",
    "    posts[\"title\"] = stemming(t_remove)\n",
    "    posts[\"body\"] = stemming(b_remove)\n",
    "    posts[\"comments\"] = []\n",
    "    for i in c_remove:\n",
    "    \ta = stemming(i)\n",
    "    \tposts[\"comments\"].append(a)\n",
    "    #print(posts[\"title\"])\n",
    "    #posts[\"comments\"] =  stemming(c_remove)\n",
    "    if(posts[\"comments\"] != []):\n",
    "        posts[\"combine\"] = posts[\"title\"]+ posts[\"body\"]+ posts[\"comments\"][0]\n",
    "    else:\n",
    "        posts[\"combine\"] = posts[\"title\"]+ posts[\"body\"]\n",
    "    loaded_model = pickle.load(open(\"finalized_model.sav\", 'rb'))\n",
    "    return loaded_model.predict(posts['combine'])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a url\n",
      "Photography\n",
      "Coronavirus\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter a url\")\n",
    "url = \"https://www.reddit.com/r/india/comments/g1qttv/watch_moradabad_some_people_pelted_stones_at/\"\n",
    "print(predict(url))\n",
    "print(actual(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
